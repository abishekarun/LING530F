{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">LING530F: Deep Learning for NLP</h1>\n",
    "<h1 align=\"center\">Assignment 2 : Implicit Emotion Detection</h1>\n",
    "<h1 align=\"center\">Convolutional Neural Network(CNN) Model</h1>\n",
    "<h2 align=\"center\"> Arun Rajendran(86611860)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9uag8ljF07Og"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
    "!pip3 install torchvision\n",
    "!pip install GoogleDriveDownloader\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch\n",
    "!pip install torchtext\n",
    "!pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N43Fyhrsbfvq"
   },
   "outputs": [],
   "source": [
    "# Download the data\n",
    "\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='11surQQr3jbmzHDKgEA5c46CKA23Nf7PA',                                 # Id of file to be downloaded\n",
    "                                    dest_path='data/dev.csv')                                                    # Destination path\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1TokPKns1uZRAW-GDN0fUz259haU2ZZAW',                                 # Id of file to be downloaded\n",
    "                                    dest_path='data/test.csv')                                                    # Destination path\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1oBiUaUHLrXehWF570xLyQNTjD9QU4fPY',                                 # Id of file to be downloaded\n",
    "                                    dest_path='data/train.csv')                                                    # Destination path\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1ewgcXWBioeMVJlKsZK9gLv3hWxGDqmiz',                                 # Id of file to be downloaded\n",
    "                                    dest_path='data/trial-v3.labels')                                                    # Destination path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbdcZz_OblvN"
   },
   "outputs": [],
   "source": [
    "# Replace the fake labels in dev file with correct labels\n",
    "\n",
    "labels = open('data/trial-v3.labels','r').readlines()\n",
    "lines = open('data/dev.csv','r').readlines()\n",
    "correct_lines=[]\n",
    "for label, line in zip(labels, lines):\n",
    "        line=line.rstrip('\\n')\n",
    "        text = line.split(',')\n",
    "        text[0]=label\n",
    "        correct_line=[''.join(text[0]),''.join(text[1:])]\n",
    "        correct_lines.append(correct_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xxNVqHJIbnZO"
   },
   "outputs": [],
   "source": [
    "# Write the clean dev data in new file\n",
    "out = open('data/clean_dev.csv',mode='w')                                      \n",
    "for line in correct_lines:\n",
    "    line[0]=line[0].rstrip('\\n')\n",
    "    out.write(line[0])\n",
    "    out.write(',')\n",
    "    out.write(line[1])\n",
    "    out.write('\\n')\n",
    "out.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Riv1NfxbnfP"
   },
   "outputs": [],
   "source": [
    "# Read train and dev data\n",
    "%%capture  # As training data has some error lines\n",
    "import pandas as pd\n",
    "X_train = pd.read_table('data/train.csv',sep=',',index_col=False, error_bad_lines=False)\n",
    "X_test = pd.read_csv('data/clean_dev.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sy7Ll0CS0Rfs"
   },
   "outputs": [],
   "source": [
    "# Pandas read skips the error lines, then we save the new dataset into separate file\n",
    "\n",
    "X_train.to_csv('data/clean_train.csv',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5sACbfI17ZT"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "from skimage import transform\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter,OrderedDict\n",
    "from keras.utils.vis_utils import *\n",
    "import random\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NT9hjh34gGr9"
   },
   "outputs": [],
   "source": [
    "# Define the vector of glove embeddings\n",
    "\n",
    "%%capture  # Not display output\n",
    "from torchtext import vocab\n",
    "# specify the path to the localy saved vectors\n",
    "vec = vocab.GloVe(name='840B', dim=300)\n",
    "# vec = vocab.GloVe(name='6B', dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28EUVijcR5bD"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import spacy\n",
    "import re\n",
    "from torchtext import data\n",
    "\n",
    "# tokenizer function using spacy\n",
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s))]\n",
    "\n",
    "def tweet_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text.strip()\n",
    "\n",
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s))]\n",
    "\n",
    "# define the columns that we want to process and how to process\n",
    "TEXT = data.Field(sequential=True, \n",
    "                 tokenize=tokenizer, \n",
    "                 include_lengths=True, \n",
    "                 use_vocab=True)\n",
    "LABEL = data.Field(sequential=False, \n",
    "                   use_vocab=True,\n",
    "                   pad_token=None, \n",
    "                   unk_token=None)\n",
    "\n",
    "train_val_fields = [\n",
    "    ('label', LABEL), # process it as label\n",
    "    ('tweet', TEXT) # process it as text\n",
    "]\n",
    "\n",
    "# Defining the data as torchtext tabular dataset\n",
    "trainds, valds = data.TabularDataset.splits(path='data', \n",
    "                                            format='csv', \n",
    "                                            train='clean_train.csv', \n",
    "                                            validation='clean_dev.csv', \n",
    "                                            fields=train_val_fields, \n",
    "                                            skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73zEBx4_lQwT"
   },
   "outputs": [],
   "source": [
    "# build the vocabulary using train and validation dataset and assign the vectors\n",
    "TEXT.build_vocab(trainds,valds, max_size=100000, vectors=vec)\n",
    "# build vocab for labels\n",
    "LABEL.build_vocab(trainds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Q05tUifp0ne0",
    "outputId": "5050b661-1873-4cd3-e7f3-c8b67e054c2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.6339,  0.1782, -0.5805,  ...,  0.3635, -0.3022, -0.0209],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8860, -0.0697, -0.3037,  ...,  0.4366, -0.3720, -0.0778]])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PHmzIO3_85sI",
    "outputId": "e295f141-1fca-444b-f133-0819998063cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "DWGB6DPD0tup",
    "outputId": "f104019e-798d-491e-ac94-a1bb0c97443c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1463, -0.2839,  0.1000,  0.2137, -0.1708,  0.0587, -0.1666, -0.2774,\n",
       "         0.4632,  2.3923, -0.5996,  0.0891, -0.4056,  0.1393, -0.2873, -0.0107,\n",
       "         0.0960,  1.3774, -0.2206,  0.0566,  0.2584, -0.1883, -0.4987, -0.0209,\n",
       "        -0.2635, -0.3824, -0.2033,  0.2165,  0.2553, -0.1092, -0.2142, -0.3225,\n",
       "        -0.2130,  0.1027, -0.1276, -0.5502, -0.3505,  0.2561, -0.1692, -0.1331,\n",
       "         0.0854,  0.4556,  0.2311, -0.0112,  0.1388, -0.1788, -0.3458, -0.0037,\n",
       "        -0.1015,  0.4884,  0.1791,  0.1711, -0.6237,  0.2748, -0.1450,  0.1254,\n",
       "         0.0029, -0.1039,  0.2158, -0.0774,  0.1305, -0.1567,  0.2946,  0.0614,\n",
       "        -0.2103, -0.0955, -0.3814,  0.2681, -0.3804, -0.0691, -0.4152, -0.3095,\n",
       "         0.3485,  0.2826,  0.4328, -0.0420,  0.2950, -0.1115,  0.0288, -0.0221,\n",
       "         0.0126,  0.4712, -0.3743, -0.3939, -0.0773, -0.0103,  0.1367, -0.3248,\n",
       "         0.4270,  0.4282, -0.0162,  0.0205,  0.4585, -0.2619,  0.3890, -0.1342,\n",
       "         0.2487,  0.1092,  0.2860, -0.4007, -0.1990,  0.3210, -0.3516,  0.0421,\n",
       "         0.4703, -0.4109, -0.3295, -0.3202,  0.0871, -0.5113, -0.1093,  0.0838,\n",
       "        -0.0432, -0.6636, -0.4697, -0.3676, -0.0559, -0.2691,  0.0257,  0.2409,\n",
       "        -0.3312,  0.0073,  0.3869, -0.0094, -0.5759, -0.0705, -0.0714, -0.0639,\n",
       "        -0.1381,  0.2430, -0.1199, -0.2121, -0.1910, -0.1342,  0.6535,  0.0540,\n",
       "        -0.2010,  0.1758,  0.1041,  0.1803, -0.2568, -0.2506,  0.1519, -0.1284,\n",
       "         0.2441,  0.3163,  0.1698, -0.5239, -0.0736, -0.0658,  0.0943, -0.3120,\n",
       "        -0.1569, -0.0128,  0.0511, -0.5127, -0.3751,  0.2616,  0.0164,  0.8343,\n",
       "        -0.2029,  0.1521,  0.3927,  0.1816, -0.5192, -0.2707,  0.5551, -0.4530,\n",
       "         0.0321,  0.1356,  0.2708,  0.2278,  0.1610,  0.0138,  0.1645,  0.4215,\n",
       "        -0.0704,  0.0329,  0.1193,  0.1916, -0.2221,  0.0154, -0.1053, -0.0033,\n",
       "        -0.3519, -0.1283, -0.1786,  0.3933,  0.4533,  0.1045,  0.0371, -0.2183,\n",
       "        -0.5497, -0.1748,  0.0859,  0.6979,  0.1982,  0.0908,  0.0050, -0.1084,\n",
       "        -0.2463,  0.1347, -0.0923,  0.0632,  0.0641, -0.0206,  0.3602, -0.2258,\n",
       "        -0.2953, -0.3653, -0.1890,  0.0637,  0.0127, -0.2887, -0.0825,  0.4561,\n",
       "        -0.3359, -0.1272, -0.0192,  0.3273,  0.0278,  0.1235, -0.1411,  0.0810,\n",
       "        -0.0729, -0.5840, -0.1841, -0.2821, -0.0503,  0.0408,  0.1639,  0.2168,\n",
       "         0.1537,  0.2852, -0.1329, -0.8017, -0.1219,  0.1062,  0.3846, -0.1927,\n",
       "         0.1629,  0.1012,  0.2149,  0.3489,  0.1709,  0.0869, -0.0847, -0.0336,\n",
       "        -0.5952,  0.2575,  0.4053, -0.0653, -0.2339,  0.0966,  0.2588,  0.8673,\n",
       "         0.2344,  0.5257,  0.1927, -0.3767,  0.4968,  0.6171,  0.2195,  0.3477,\n",
       "        -0.0419, -0.0952, -0.2188, -0.4997,  0.0508, -0.3474,  0.9415, -0.1533,\n",
       "        -0.3785, -0.4441, -0.2830, -0.3975,  0.2100,  0.0953,  0.3609,  0.1585,\n",
       "         0.3580,  0.0304, -0.0024,  0.0111, -0.1433, -0.1992, -0.2572,  0.1572,\n",
       "         0.3300, -0.4173, -0.4734,  0.1855,  0.4898, -0.7664, -0.1251,  0.0139,\n",
       "         0.3202, -0.0200,  0.4843, -0.2722])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors[TEXT.vocab.stoi['under']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9afOJyTKk8Rv"
   },
   "outputs": [],
   "source": [
    "# Create bucketiterator to get batches\n",
    "\n",
    "traindl, valdl = data.BucketIterator.splits(datasets=(trainds, valds), # specify train and validation Tabulardataset\n",
    "                                            batch_sizes=(64,64),  # batch size of train and validation\n",
    "                                            sort_key=lambda x: len(x.tweet), # on what attribute the text should be sorted\n",
    "                                            device=None, # -1 mean cpu and 0 or None mean gpu\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N6ZGgIxc39A2",
    "outputId": "2d353da8-12d1-4672-aebd-815c0b68903d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2365, 150)\n"
     ]
    }
   ],
   "source": [
    "print(len(traindl), len(valdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t9mIWH8v9Tcr",
    "outputId": "74c875cc-4914-43ce-be37-528f8a7dfd32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.iterator.BucketIterator at 0x7f645409d310>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4tZudVU3_OG"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(traindl)) # BucketIterator return a batch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "PNKXAgZB4A4X",
    "outputId": "86caef58-996b-4084-ddca-a0fbcf851374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 3, 4, 0, 1, 1, 4, 0, 4, 2, 1, 2, 1, 1, 2, 3, 2, 2, 3, 0, 0, 1, 4,\n",
      "        0, 3, 4, 1, 4, 3, 1, 0, 4, 0, 1, 0, 1, 0, 5, 4, 0, 1, 1, 3, 3, 3, 3, 1,\n",
      "        4, 1, 2, 1, 2, 1, 3, 1, 4, 0, 0, 1, 5, 3, 4, 0])\n"
     ]
    }
   ],
   "source": [
    "print(batch.label) # labels of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sfvbus18TAhT",
    "outputId": "13a86062-1f94-4527-8068-b539c0f0b6ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(batch.label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6NXtkq46TCve",
    "outputId": "d4eb8ef5-9e5a-4a5f-8cf3-fbb02234527d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 64])\n"
     ]
    }
   ],
   "source": [
    "(x,y) = batch.tweet\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "HfWglGms4CHK",
    "outputId": "7e2aef41-90e9-4230-f388-4ad394d3b33a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[    5,     5,     5,  ...,    33,     5,     5],\n",
      "        [  383,   482,     3,  ...,    59,     5,     3],\n",
      "        [    3,     3,    71,  ...,    85,     3,    21],\n",
      "        ...,\n",
      "        [28504,    34,    69,  ...,    18,  1323,    34],\n",
      "        [  115,   314,  1834,  ...,    14,  9747,    12],\n",
      "        [    5,  8431, 64060,  ...,  1138,   495,   298]]), tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22, 22, 22, 22, 22]))\n"
     ]
    }
   ],
   "source": [
    "print(batch.tweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XQkVolv04Dfs",
    "outputId": "ca1d795e-fda0-47dd-ec07-623d49e5c4e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet': <torchtext.data.field.Field object at 0x7f64744e4cd0>, 'label': <torchtext.data.field.Field object at 0x7f6432d036d0>}\n"
     ]
    }
   ],
   "source": [
    "print(batch.dataset.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssMm5Wp54E2D"
   },
   "outputs": [],
   "source": [
    "# Create batchgenerator class to use Bucketiterator and attributes to get the batches\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, dl, x_field, y_field):\n",
    "        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            X = getattr(batch, self.x_field)\n",
    "            y = getattr(batch, self.y_field)\n",
    "            yield (X,y)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJ1TfTlA4F7S"
   },
   "outputs": [],
   "source": [
    "train_batch_it = BatchGenerator(traindl, 'tweet', 'label')\n",
    "valid_batch_it = BatchGenerator(valdl, 'tweet', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6LKOxYP7kHQ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    " \n",
    "# Define the CNN class    \n",
    "    \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, emb_dim, kernel_sizes,kernel_dim,dropout_p):\n",
    "        super(SimpleCNN,self).__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors, requires_grad=False)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernel_dim, (kernel_sizes[0],emb_dim)),\n",
    "            nn.BatchNorm2d(kernel_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout_p))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernel_dim, (kernel_sizes[1],emb_dim)),\n",
    "            nn.BatchNorm2d(kernel_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout_p))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernel_dim, (kernel_sizes[2],emb_dim)),\n",
    "            nn.BatchNorm2d(kernel_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout_p))\n",
    "\n",
    "        # kernal_size = (K,D) \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(len(kernel_sizes) * kernel_dim, 6)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        seq = self.embedding(seq).unsqueeze(1)\n",
    "        inputs=[]\n",
    "        inputs.append(self.layer1(seq).squeeze(3))\n",
    "        inputs.append(self.layer2(seq).squeeze(3))\n",
    "        inputs.append(self.layer3(seq).squeeze(3))\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] \n",
    "\n",
    "        concated = torch.cat(inputs, 1)\n",
    "        concated = self.dropout(concated)\n",
    "        out = self.fc1(concated)\n",
    "        return F.log_softmax(out,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PObJ6utqAtNF"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "kernel_dim = 256\n",
    "kernel_size = [3,4,5]\n",
    "drop=0.5\n",
    "model = SimpleCNN(embed_size,kernel_size,kernel_dim,drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "hKn5fzsc_DQO",
    "outputId": "7e0a52eb-1abd-4ba8-c4f8-338011bcaa60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1.0131, Validation Loss: 1.0421, F1 Sum:88.5065,Average F1 score(Validation Set): 0.5940\n",
      "Epoch: 2, Training Loss: 0.9980, Validation Loss: 1.0479, F1 Sum:88.5103,Average F1 score(Validation Set): 0.5940\n",
      "Epoch: 3, Training Loss: 0.9493, Validation Loss: 1.0362, F1 Sum:88.6863,Average F1 score(Validation Set): 0.5952\n",
      "Epoch: 4, Training Loss: 0.9318, Validation Loss: 1.0348, F1 Sum:88.8635,Average F1 score(Validation Set): 0.5964\n",
      "Epoch: 5, Training Loss: 0.9207, Validation Loss: 1.0333, F1 Sum:89.0771,Average F1 score(Validation Set): 0.5978\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64 \n",
    "scheduler = optim.lr_scheduler.StepLR(opt, step_size=2, gamma=0.1)\n",
    "  \n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    iterations=0\n",
    "    scheduler.step()\n",
    "    model.train() # turn on training mode\n",
    "    for x,y in tqdm(train_batch_it,disable=True): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        iterations=iterations+1\n",
    "        opt.zero_grad()\n",
    "        (x,lengths)=x\n",
    "        x=x.transpose(0,1)\n",
    "        predicted = model(x)\n",
    "        y=y.view(-1)\n",
    "        loss = loss_func(predicted,y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    epoch_loss = running_loss / len(trainds)\n",
    " \n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    output=[]\n",
    "    target=[]\n",
    "    list_scores=[]\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x,y in tqdm(valid_batch_it,disable=True):\n",
    "        (x,lengths)=x\n",
    "        x=x.transpose(0,1)\n",
    "        preds = model(x)\n",
    "        y=y.view(-1)\n",
    "        loss = loss_func(preds,y)\n",
    "        val_loss += loss.item() * x.size(0)\n",
    "        _,y_pred=torch.max(preds,1)\n",
    "        output.append(y_pred)\n",
    "        target.append(y)\n",
    "        temp=f1_score(y_pred,y, average='macro', labels=np.unique(y_pred))\n",
    "        list_scores.append(temp)\n",
    "    \n",
    "    val_loss /= len(valds)\n",
    "    fscore = sum(list_scores)/(len(valds)/batch_size)\n",
    "    torch.save(model.state_dict(), 'mytraining'+str(epoch)+'.pt')\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, F1 Sum:{:.4f},Average F1 score(Validation Set): {:.4f}'.format(epoch, epoch_loss, val_loss,sum(list_scores),fscore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ff1Bpy2kPUv6"
   },
   "outputs": [],
   "source": [
    "# Save the model for further training/evaluation\n",
    "\n",
    "from google.colab import files\n",
    "files.download('mytraining5.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "NLTK: Stemming and Lemmatization<br\\>\n",
    "https://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    "\n",
    "Apply Stemmer to a column<br\\>\n",
    "https://stackoverflow.com/questions/43795310/apply-porters-stemmer-to-a-pandas-column-for-each-word\n",
    "\n",
    "Ensemble: Voting Classifier<br\\>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "\n",
    "Sample Pipeline<br\\>\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "\n",
    "TorchText and LSTM<br\\>\n",
    "http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "\n",
    "TorchText and CNN<br\\>\n",
    "https://github.com/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/08.CNN-for-Text-Classification.ipynb\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "\n",
    "Saving and loading Pytorch Model<br\\>\n",
    "https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
    "\n",
    "Self Attention Explained<br\\>\n",
    "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#self-attention\n",
    "\n",
    "Self Attention Code for LSTM<br\\>\n",
    "https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/models/LSTM_Attn.py\n",
    "\n",
    "LSTM + CNN model \n",
    "https://discuss.pytorch.org/t/cnn-layer-in-the-top-of-lstm/7941/5\n",
    "https://discuss.pytorch.org/t/solved-concatenate-time-distributed-cnn-with-lstm/15435/4"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
