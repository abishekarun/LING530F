{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FkDWO3cLXAN"
   },
   "source": [
    "<h1 align=\"center\">LING530F: Deep Learning for NLP</h1>\n",
    "<h1 align=\"center\">Assignment 2 : Implicit Emotion Detection</h1>\n",
    "<h1 align=\"center\">LSTM + CNN model</h1>\n",
    "<h2 align=\"center\"> Arun Rajendran(86611860)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9uag8ljF07Og"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
    "!pip3 install torchvision\n",
    "!pip install GoogleDriveDownloader\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch\n",
    "!pip install torchtext\n",
    "!pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N43Fyhrsbfvq"
   },
   "outputs": [],
   "source": [
    "# Download the data\n",
    "\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='11surQQr3jbmzHDKgEA5c46CKA23Nf7PA',                                 # Id of file to be downloaded\n",
    "                                    dest_path='data/dev.csv')                                                    # Destination path\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1TokPKns1uZRAW-GDN0fUz259haU2ZZAW',                                 # Id of file to be downloaded\n",
    "                                    dest_path='data/test.csv')                                                    # Destination path\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1oBiUaUHLrXehWF570xLyQNTjD9QU4fPY',                                 # Id of file to be downloaded\n",
    "                                    dest_path='data/train.csv')                                                    # Destination path\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1ewgcXWBioeMVJlKsZK9gLv3hWxGDqmiz',                                 # Id of file to be downloaded\n",
    "                                    dest_path='data/trial-v3.labels')                                                    # Destination path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbdcZz_OblvN"
   },
   "outputs": [],
   "source": [
    "# Replace the fake labels in dev file with correct labels\n",
    "\n",
    "labels = open('data/trial-v3.labels','r').readlines()\n",
    "lines = open('data/dev.csv','r').readlines()\n",
    "correct_lines=[]\n",
    "for label, line in zip(labels, lines):\n",
    "        line=line.rstrip('\\n')\n",
    "        text = line.split(',')\n",
    "        text[0]=label\n",
    "        correct_line=[''.join(text[0]),''.join(text[1:])]\n",
    "        correct_lines.append(correct_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xxNVqHJIbnZO"
   },
   "outputs": [],
   "source": [
    "# Write the clean dev data in new file\n",
    "out = open('data/clean_dev.csv',mode='w')                                      \n",
    "for line in correct_lines:\n",
    "    line[0]=line[0].rstrip('\\n')\n",
    "    out.write(line[0])\n",
    "    out.write(',')\n",
    "    out.write(line[1])\n",
    "    out.write('\\n')\n",
    "out.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Riv1NfxbnfP"
   },
   "outputs": [],
   "source": [
    "# Read train and dev data\n",
    "%%capture  # As training data has some error lines\n",
    "import pandas as pd\n",
    "X_train = pd.read_table('data/train.csv',sep=',',index_col=False, error_bad_lines=False)\n",
    "X_test = pd.read_csv('data/clean_dev.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sy7Ll0CS0Rfs"
   },
   "outputs": [],
   "source": [
    "# Pandas read skips the error lines, then we save the new dataset into separate file\n",
    "\n",
    "X_train.to_csv('data/clean_train.csv',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y5sACbfI17ZT",
    "outputId": "c1e2c9d9-e165-406f-c993-64c2a1bf219e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "from skimage import transform\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter,OrderedDict\n",
    "from keras.utils.vis_utils import *\n",
    "import random\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NT9hjh34gGr9"
   },
   "outputs": [],
   "source": [
    "# Define the vector of glove embeddings\n",
    "\n",
    "%%capture  # Not display output\n",
    "from torchtext import vocab\n",
    "# specify the path to the localy saved vectors\n",
    "vec = vocab.GloVe(name='840B', dim=300)\n",
    "# vec = vocab.GloVe(name='6B', dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28EUVijcR5bD"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import spacy\n",
    "import re\n",
    "from torchtext import data\n",
    "\n",
    "# tokenizer function using spacy\n",
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s))]\n",
    "\n",
    "def tweet_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text.strip()\n",
    "\n",
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s))]\n",
    "\n",
    "# define the columns that we want to process and how to process\n",
    "TEXT = data.Field(sequential=True, \n",
    "                 tokenize=tokenizer, \n",
    "                 include_lengths=True, \n",
    "                 use_vocab=True)\n",
    "LABEL = data.Field(sequential=False, \n",
    "                   use_vocab=True,\n",
    "                   pad_token=None, \n",
    "                   unk_token=None)\n",
    "\n",
    "train_val_fields = [\n",
    "    ('label', LABEL), # process it as label\n",
    "    ('tweet', TEXT) # process it as text\n",
    "]\n",
    "\n",
    "trainds, valds = data.TabularDataset.splits(path='data', \n",
    "                                            format='csv', \n",
    "                                            train='clean_train.csv', \n",
    "                                            validation='clean_dev.csv', \n",
    "                                            fields=train_val_fields, \n",
    "                                            skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73zEBx4_lQwT"
   },
   "outputs": [],
   "source": [
    "# build the vocabulary using train and validation dataset and assign the vectors\n",
    "TEXT.build_vocab(trainds,valds, max_size=100000, vectors=vec)\n",
    "# build vocab for labels\n",
    "LABEL.build_vocab(trainds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Q05tUifp0ne0",
    "outputId": "873299be-77dc-4e77-9335-979776437505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.6339,  0.1782, -0.5805,  ...,  0.3635, -0.3022, -0.0209],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8860, -0.0697, -0.3037,  ...,  0.4366, -0.3720, -0.0778]])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PHmzIO3_85sI",
    "outputId": "82915b1a-e25d-4c96-a77e-c36a3ad62f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "DWGB6DPD0tup",
    "outputId": "f3f137d0-c940-4aae-9df9-27de1be2475c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1463, -0.2839,  0.1000,  0.2137, -0.1708,  0.0587, -0.1666, -0.2774,\n",
       "         0.4632,  2.3923, -0.5996,  0.0891, -0.4056,  0.1393, -0.2873, -0.0107,\n",
       "         0.0960,  1.3774, -0.2206,  0.0566,  0.2584, -0.1883, -0.4987, -0.0209,\n",
       "        -0.2635, -0.3824, -0.2033,  0.2165,  0.2553, -0.1092, -0.2142, -0.3225,\n",
       "        -0.2130,  0.1027, -0.1276, -0.5502, -0.3505,  0.2561, -0.1692, -0.1331,\n",
       "         0.0854,  0.4556,  0.2311, -0.0112,  0.1388, -0.1788, -0.3458, -0.0037,\n",
       "        -0.1015,  0.4884,  0.1791,  0.1711, -0.6237,  0.2748, -0.1450,  0.1254,\n",
       "         0.0029, -0.1039,  0.2158, -0.0774,  0.1305, -0.1567,  0.2946,  0.0614,\n",
       "        -0.2103, -0.0955, -0.3814,  0.2681, -0.3804, -0.0691, -0.4152, -0.3095,\n",
       "         0.3485,  0.2826,  0.4328, -0.0420,  0.2950, -0.1115,  0.0288, -0.0221,\n",
       "         0.0126,  0.4712, -0.3743, -0.3939, -0.0773, -0.0103,  0.1367, -0.3248,\n",
       "         0.4270,  0.4282, -0.0162,  0.0205,  0.4585, -0.2619,  0.3890, -0.1342,\n",
       "         0.2487,  0.1092,  0.2860, -0.4007, -0.1990,  0.3210, -0.3516,  0.0421,\n",
       "         0.4703, -0.4109, -0.3295, -0.3202,  0.0871, -0.5113, -0.1093,  0.0838,\n",
       "        -0.0432, -0.6636, -0.4697, -0.3676, -0.0559, -0.2691,  0.0257,  0.2409,\n",
       "        -0.3312,  0.0073,  0.3869, -0.0094, -0.5759, -0.0705, -0.0714, -0.0639,\n",
       "        -0.1381,  0.2430, -0.1199, -0.2121, -0.1910, -0.1342,  0.6535,  0.0540,\n",
       "        -0.2010,  0.1758,  0.1041,  0.1803, -0.2568, -0.2506,  0.1519, -0.1284,\n",
       "         0.2441,  0.3163,  0.1698, -0.5239, -0.0736, -0.0658,  0.0943, -0.3120,\n",
       "        -0.1569, -0.0128,  0.0511, -0.5127, -0.3751,  0.2616,  0.0164,  0.8343,\n",
       "        -0.2029,  0.1521,  0.3927,  0.1816, -0.5192, -0.2707,  0.5551, -0.4530,\n",
       "         0.0321,  0.1356,  0.2708,  0.2278,  0.1610,  0.0138,  0.1645,  0.4215,\n",
       "        -0.0704,  0.0329,  0.1193,  0.1916, -0.2221,  0.0154, -0.1053, -0.0033,\n",
       "        -0.3519, -0.1283, -0.1786,  0.3933,  0.4533,  0.1045,  0.0371, -0.2183,\n",
       "        -0.5497, -0.1748,  0.0859,  0.6979,  0.1982,  0.0908,  0.0050, -0.1084,\n",
       "        -0.2463,  0.1347, -0.0923,  0.0632,  0.0641, -0.0206,  0.3602, -0.2258,\n",
       "        -0.2953, -0.3653, -0.1890,  0.0637,  0.0127, -0.2887, -0.0825,  0.4561,\n",
       "        -0.3359, -0.1272, -0.0192,  0.3273,  0.0278,  0.1235, -0.1411,  0.0810,\n",
       "        -0.0729, -0.5840, -0.1841, -0.2821, -0.0503,  0.0408,  0.1639,  0.2168,\n",
       "         0.1537,  0.2852, -0.1329, -0.8017, -0.1219,  0.1062,  0.3846, -0.1927,\n",
       "         0.1629,  0.1012,  0.2149,  0.3489,  0.1709,  0.0869, -0.0847, -0.0336,\n",
       "        -0.5952,  0.2575,  0.4053, -0.0653, -0.2339,  0.0966,  0.2588,  0.8673,\n",
       "         0.2344,  0.5257,  0.1927, -0.3767,  0.4968,  0.6171,  0.2195,  0.3477,\n",
       "        -0.0419, -0.0952, -0.2188, -0.4997,  0.0508, -0.3474,  0.9415, -0.1533,\n",
       "        -0.3785, -0.4441, -0.2830, -0.3975,  0.2100,  0.0953,  0.3609,  0.1585,\n",
       "         0.3580,  0.0304, -0.0024,  0.0111, -0.1433, -0.1992, -0.2572,  0.1572,\n",
       "         0.3300, -0.4173, -0.4734,  0.1855,  0.4898, -0.7664, -0.1251,  0.0139,\n",
       "         0.3202, -0.0200,  0.4843, -0.2722])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors[TEXT.vocab.stoi['under']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9afOJyTKk8Rv"
   },
   "outputs": [],
   "source": [
    "# Create bucketiterator to get batches\n",
    "\n",
    "traindl, valdl = data.BucketIterator.splits(datasets=(trainds, valds), # specify train and validation Tabulardataset\n",
    "                                            batch_sizes=(64,64),  # batch size of train and validation\n",
    "                                            sort_key=lambda x: len(x.tweet), # on what attribute the text should be sorted\n",
    "                                            device=None, # -1 mean cpu and 0 or None mean gpu\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N6ZGgIxc39A2",
    "outputId": "7da449f3-8a7b-43f2-9191-6e3df916fcf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2365, 150)\n"
     ]
    }
   ],
   "source": [
    "print(len(traindl), len(valdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t9mIWH8v9Tcr",
    "outputId": "0422bd84-6fcf-46fe-c342-cc4fcd53e6cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.iterator.BucketIterator at 0x7f9089605c90>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4tZudVU3_OG"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(traindl)) # BucketIterator return a batch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "PNKXAgZB4A4X",
    "outputId": "c84c92ac-7d89-431f-85d4-983483e91469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 2, 2, 1, 0, 2, 2, 1, 5, 2, 4, 2, 1, 4, 4, 0, 0, 2, 5, 1, 3, 0, 3, 1,\n",
      "        2, 0, 1, 1, 2, 2, 1, 1, 4, 2, 3, 5, 3, 2, 5, 0, 2, 2, 5, 0, 0, 3, 5, 2,\n",
      "        0, 3, 5, 2, 2, 5, 0, 5, 2, 5, 2, 1, 4, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(batch.label) # labels of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1853
    },
    "colab_type": "code",
    "id": "HfWglGms4CHK",
    "outputId": "74ce6e04-4adf-4036-bdf0-eee374edb84d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  896,     5,  3850,     3,    10,     3,     5,     5,    33,     5,\n",
      "             3,    10,    20,  1691,     5,     9,     3,     5,     3,     5,\n",
      "          3979,   117,     5,     9,   988,     3,     3,   106,    10,     5,\n",
      "             9,     3,     3,     5,  1710,    54,    33,     5,    16,  1397,\n",
      "             3,     3,    14,  1297,     5,    68,     5,     3, 48805, 27757,\n",
      "             5,     5,     5,     5,     5,     9,    20,   283,  2539,     5,\n",
      "             5,     5,     5,     5],\n",
      "        [ 1815,  1120,     5,    48,    58,    43,   123,     5,    60,    32,\n",
      "            31,    49,   528,   226,     3,   206,    11,     5,   248,   106,\n",
      "            50,   448,     5,    15,     8,    11,    31,    14,    56,     5,\n",
      "            16,    31,    11,   268,    21,     2,    60,     5,     9,  1393,\n",
      "            11,    11,     2,   707,     5,     2,    14,   126,     3,    15,\n",
      "             5,     5,     3,     9,     3,    15,   791, 66404, 54930,     5,\n",
      "            94,    41,     6,     2],\n",
      "        [    3,    24, 18002,   550,     3,    18,  3975,     5,   130,     2,\n",
      "            14,     2,   129,   185,    11,    60,     2,     9,   354,   217,\n",
      "             6,     3,     5,   179,    12,     2,    14,     2,    23,     5,\n",
      "            14,     2,     2,    10,     2,    17,    24,   242,     2,     8,\n",
      "            89,  2091,     4,   144,    13,     4,     2,     9,    11, 34770,\n",
      "           785, 68716,    11,    15,    11,     2,    21,  3438,    21,     3,\n",
      "         11862,    10,  5732,     4],\n",
      "        [   11,     2,    50,     2,    11,   337,   142,     3,    14,     7,\n",
      "             2,     4,     3,    13,     2,    24,     4,    15,    50,     2,\n",
      "           274,    11,     3,     2,   378,    17,     2,     4,     2,    13,\n",
      "             2,     7,     4,   391,     7,     3,  4721,   176,     4,    23,\n",
      "             2,     2,     9,    24,    48,    44,     4,    54,   623,    19,\n",
      "             6,    16,     2,     2,    14,     7,     2,   113,     2,    48,\n",
      "            16,    93,    93,    35],\n",
      "        [    2,     4, 49730,     4,     2,     3,   140,    11,     2,    14,\n",
      "             7,   314,    21,     3,     4,    14,     3,     2,     5,     7,\n",
      "           398,    95,    11,     4,   211,     3,     7,   111,     7,   115,\n",
      "             7,    12,   222,     6,    30,    45,     2,    18,     3,     2,\n",
      "             7,     4,   648,     2,     2,   103, 56898,     2,  2245,     6,\n",
      "          1430,    32,     4,     4,     2,   773,     7,    97,     4,   642,\n",
      "            34,     2,  4013,  1932],\n",
      "        [    4,  7394, 25411,    76,     7,    11,    23,    68,    17,   159,\n",
      "             3,    35,     2,    11,    15,     2,    48,    17,  4874,    10,\n",
      "           978,    73,     2,    72,     3,    45,   185,   417,    10,  6103,\n",
      "            35,   690,    56, 34585,  1048,    18,    17,     2,    11,     4,\n",
      "            35,    33,    53,    17,     4,    61,    15,     7,   393, 14185,\n",
      "           629,     2,    10,    44,     4,    58,    47,    24,     3,     2,\n",
      "          8938,     4,     2,   138],\n",
      "        [   70,  5302,  1849,    86,     3,  2519,     2,     2,     9,    25,\n",
      "            31,    25,    17,    68,    66,     4,   113,     6,    13,   237,\n",
      "            13,     2,     4,    15,    11,    18,   755,   605,    63,     3,\n",
      "          2302,   121,   155,    42,    19,    31,    26,    17,  1090,  7061,\n",
      "           182,   200,    33,     4,   349,     8,   475,    35,    36,    81,\n",
      "           440,     4,   149,    50,    10,    57,   234,   920,   102,     4,\n",
      "            78,    30,     7,   283],\n",
      "        [    3,   170,    22,  1521,    31,     2,     7,     4,   454,    12,\n",
      "            12,  2862,     3,     2,     3,    20,    22, 22368,     9, 13233,\n",
      "             2,     7, 15104,    35,     2,    38,  2128,     8,     6,   292,\n",
      "            42,  1415,    36,    51,     6,     8,    25,   193,    13,    43,\n",
      "            24,    18,   132,   504,     8,    63,    25,   116,  7333,    25,\n",
      "          2275,    33,   116,   408,    49,    62,    53,     2,   884,     5,\n",
      "            25,   149,    26,    12],\n",
      "        [   87,    18,  7995,    88,  1176,    17,     6, 10336,    46,   241,\n",
      "          1175,    50,   129,     7,    11,   148, 51949,    93,    60,   864,\n",
      "             4,     3,    15, 27451,     7,     8,    50,   181,  2046,  3915,\n",
      "            69,    68,     3,   861,  1053,   430,    14,  3092,   312,   497,\n",
      "            26,    89,  3054,  3766,    24, 44194,    82,     4,    14,    35,\n",
      "            31,  1585,    33,  2335,   315,    36,     3,     4,   141,    16,\n",
      "             2,    84,  6586,   227],\n",
      "        [   10,   317,    93,     6,    19,     6, 21062,    68,   109,   698,\n",
      "           330,    12,     3,    20,   142,   144, 52916,   104,    24,   496,\n",
      "            10,    11,   889,   462,    12,  3534,    20,     8,  4434,   341,\n",
      "           671,    23,    11,   232,    19,    55, 22665,  6524,    18,   169,\n",
      "           273,    12,    34,   184,    46,   396, 13854,    26,     2,     2,\n",
      "             2,  8291,     7,     8,   195,    10,  4955,    26,    36, 21690,\n",
      "             4,   353,    33,    69],\n",
      "        [   10,     4,     2, 30874,    12,  4200,   794,   312,   777,   138,\n",
      "            43,  4303,   361,   203,   140,  1017,  7394, 12236,     2,     8,\n",
      "            49,    32,   940,   121, 11326,    37,   329,    39,    50,     2,\n",
      "           457,  1159,  5667,     2, 17951,    42,    13,    25,   105,  1442,\n",
      "            80,   180,    75,   156,     3,    83,    17,   110,    17,     7,\n",
      "             7,    85,    10,   808,     3,    68,    62,    49,     3,  7669,\n",
      "            26,    70,  2981,   263],\n",
      "        [  100,    64,     7,   385,  1120,   101,  9047,    18, 66285,    82,\n",
      "             3,  4810,     8,  6340,    38,    42, 51971,     7,    17,  9333,\n",
      "          1665,   143,    25,    12,   827, 22747,    13,    31,     5,     4,\n",
      "         12777,  1500,   250,     7,    40,  5641,   431,   609,    37,     5,\n",
      "            26,    40,   257,   183,   110,     5,    19,   103,    19,    26,\n",
      "            32,   105,    58,    67,    87,   301, 23186,    32,    11,  1013,\n",
      "            49,   204,    34,   738],\n",
      "        [  371,   504,    29,    66,   131,   120, 24288,  2293,    29,   538,\n",
      "           353,    29,  1006,    50,     8,    20,    29,    26,     3,    22,\n",
      "             8,    20,   602,  2629,   176,     8,     3,    66,    29,    76,\n",
      "            13,  5618,    34,    35,    29,   115,    29,     6,    12,    29,\n",
      "           813,    29,    29,   701,   133,     5,  1649,   105, 16570,    63,\n",
      "           538,  2884,     9,    65,    10,   791,    42,   406,    12,    29,\n",
      "            22,    86,     6,    18],\n",
      "        [   12,    64,    28,    12,   127,    69,    50,     8,    28,    33,\n",
      "            10,    28,  1784,    24,    52,   198,    28,  3972,    99,    51,\n",
      "            23,   139,     8,   459,    18,   732,  1032,    26,    28,   423,\n",
      "           139,   625,     6,   780,    28,    20,    28,   150,   210,    28,\n",
      "            57,    28,    28,    88,  9478,     5,    35,  6134,    13,   267,\n",
      "          1430,   156,    15,   187,    14,     4,    12,    82,   545,    28,\n",
      "             6,   338,   134,  1190],\n",
      "        [ 1981,  1133,    27,  3764,   314,   416,   209,    33,    27,  1811,\n",
      "           128,    27,  3457,   123,   109,   165,    27,    77,    59,   516,\n",
      "           348,  9177,   856,   636,  2397,   122,    67,    99,    27,    77,\n",
      "          7474,   139,   734,    10,    27,   510,    27,   674,   879,    27,\n",
      "            24,    27,    27,   152,    34,     5, 30165,   207, 24806,   377,\n",
      "           629,    37,  2138, 26894,   108,   448, 18692,     5,   109,    27,\n",
      "           233,    77,    90,    22]]), tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15]))\n"
     ]
    }
   ],
   "source": [
    "print(batch.tweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XQkVolv04Dfs",
    "outputId": "f7bf2187-d763-4e44-a68a-69a531f61906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet': <torchtext.data.field.Field object at 0x7f90bcee5cd0>, 'label': <torchtext.data.field.Field object at 0x7f90895aeed0>}\n"
     ]
    }
   ],
   "source": [
    "print(batch.dataset.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssMm5Wp54E2D"
   },
   "outputs": [],
   "source": [
    "# Create batchgenerator class to use Bucketiterator and attributes to get the batches\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, dl, x_field, y_field):\n",
    "        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            X = getattr(batch, self.x_field)\n",
    "            y = getattr(batch, self.y_field)\n",
    "            yield (X,y)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJ1TfTlA4F7S"
   },
   "outputs": [],
   "source": [
    "train_batch_it = BatchGenerator(traindl, 'tweet', 'label')\n",
    "valid_batch_it = BatchGenerator(valdl, 'tweet', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6LKOxYP7kHQ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    " \n",
    "# Define the LSTM with CNN class    \n",
    "    \n",
    "class LSTM_CNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_lstm_layers,emb_dim,kernel_dim,kernel_sizes, dropout_p):\n",
    "        super(LSTM_CNN,self).__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors, requires_grad=False)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=n_lstm_layers)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, kernel_dim, kernel_sizes[0],padding=kernel_sizes[0]//2),\n",
    "            nn.BatchNorm1d(kernel_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(1, kernel_dim, kernel_sizes[1],padding=kernel_sizes[1]//2),\n",
    "            nn.BatchNorm1d(kernel_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(1, kernel_dim, kernel_sizes[2],padding=kernel_sizes[2]//2),\n",
    "            nn.BatchNorm1d(kernel_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p))\n",
    "        self.fc1=nn.Linear(len(kernel_sizes)*hidden_dim*kernel_dim,128)\n",
    "        self.predictor=nn.Linear(128,6)\n",
    " \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.squeeze(0)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return new_hidden_state\n",
    "  \n",
    "    def forward(self, seq):\n",
    "        output, (final_hidden_state, final_cell_state) = self.encoder(self.embedding(seq))\n",
    "        output = output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\n",
    "        attn_output = self.attention_net(output, final_hidden_state)\n",
    "        attn_output= self.dropout(attn_output)\n",
    "        inputs=[]\n",
    "        inputs.append(self.layer1(attn_output.unsqueeze(1)))\n",
    "        inputs.append(self.layer2(attn_output.unsqueeze(1)))\n",
    "        inputs.append(self.layer3(attn_output.unsqueeze(1)))\n",
    "        output=torch.cat(inputs,1)\n",
    "        output = self.dropout(output)\n",
    "        output = output.view(output.shape[0],-1)\n",
    "        out = self.fc1(output)\n",
    "        out = self.predictor(output)\n",
    "        return F.log_softmax(out,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PObJ6utqAtNF"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "nh = 256\n",
    "drop=0.5\n",
    "n_lstm_layers=1\n",
    "kernel_dim = 100\n",
    "kernel_sizes=[1,3,5]\n",
    "model = LSTM_CNN(nh,n_lstm_layers,embed_size,kernel_dim,kernel_sizes,drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "hKn5fzsc_DQO",
    "outputId": "fac312aa-d739-4b26-aec0-4b87c040930f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.4918, Validation Loss: 0.4893, F1 Sum:61.3937,Average F1 score(Validation Set): 0.4120\n",
      "Epoch: 2, Training Loss: 0.3935, Validation Loss: 0.3742, F1 Sum:81.8725,Average F1 score(Validation Set): 0.5495\n",
      "Epoch: 3, Training Loss: 0.3561, Validation Loss: 0.3524, F1 Sum:84.9654,Average F1 score(Validation Set): 0.5702\n",
      "Epoch: 4, Training Loss: 0.3325, Validation Loss: 0.3303, F1 Sum:88.7580,Average F1 score(Validation Set): 0.5957\n",
      "Epoch: 5, Training Loss: 0.3124, Validation Loss: 0.3300, F1 Sum:89.0741,Average F1 score(Validation Set): 0.5978\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "rp = ReduceLROnPlateau(opt,'min',0.1,patience=1, cooldown=0)  \n",
    "epochs = 5\n",
    "batch_size = 64 \n",
    "  \n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    iterations=0\n",
    "    model.train() # turn on training mode\n",
    "    for x,y in tqdm(train_batch_it,disable=True): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        iterations=iterations+1\n",
    "        opt.zero_grad()\n",
    "        (x,lengths)=x\n",
    "        predicted = model(x)\n",
    "        y=y.view(-1)\n",
    "        loss = loss_func(predicted,y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    epoch_loss = running_loss / len(trainds)\n",
    " \n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    list_scores=[]\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x,y in tqdm(valid_batch_it,disable=True):\n",
    "        (x,lengths)=x\n",
    "        preds = model(x)\n",
    "        y=y.view(-1)\n",
    "        loss = loss_func(preds,y)\n",
    "        val_loss += loss.item() * x.size(0)\n",
    "        _,y_pred=torch.max(preds,1)\n",
    "        temp=f1_score(y_pred,y, average='macro', labels=np.unique(y_pred))\n",
    "        list_scores.append(temp)\n",
    "    \n",
    "    val_loss /= len(valds)\n",
    "    fscore = sum(list_scores)/(len(valds)/batch_size)\n",
    "    rp.step(val_loss)\n",
    "    torch.save(model.state_dict(), 'mytraining'+str(epoch)+'.pt')\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, F1 Sum:{:.4f},Average F1 score(Validation Set): {:.4f}'.format(epoch, epoch_loss, val_loss,sum(list_scores),fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "NLTK: Stemming and Lemmatization<br\\>\n",
    "https://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    "\n",
    "Apply Stemmer to a column<br\\>\n",
    "https://stackoverflow.com/questions/43795310/apply-porters-stemmer-to-a-pandas-column-for-each-word\n",
    "\n",
    "Ensemble: Voting Classifier<br\\>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "\n",
    "Sample Pipeline<br\\>\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "\n",
    "TorchText and LSTM<br\\>\n",
    "http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "\n",
    "TorchText and CNN<br\\>\n",
    "https://github.com/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/08.CNN-for-Text-Classification.ipynb\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "\n",
    "Saving and loading Pytorch Model<br\\>\n",
    "https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
    "\n",
    "Self Attention Explained<br\\>\n",
    "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#self-attention\n",
    "\n",
    "Self Attention Code for LSTM<br\\>\n",
    "https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/models/LSTM_Attn.py\n",
    "\n",
    "LSTM + CNN model \n",
    "https://discuss.pytorch.org/t/cnn-layer-in-the-top-of-lstm/7941/5\n",
    "https://discuss.pytorch.org/t/solved-concatenate-time-distributed-cnn-with-lstm/15435/4"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
