{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyvBieeaL9zc"
   },
   "source": [
    "<h1 align=\"center\">LING530F: Deep Learning for NLP</h1>\n",
    "<h1 align=\"center\">Assignment 2 : Implicit Emotion Detection</h1>\n",
    "<h1 align=\"center\">LSTM with Self Attention(Part 2) Model</h1>\n",
    "<h2 align=\"center\"> Arun Rajendran(86611860)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9uag8ljF07Og"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
    "!pip3 install torchvision\n",
    "!pip install GoogleDriveDownloader\n",
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch\n",
    "!pip install torchtext\n",
    "!pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "N43Fyhrsbfvq",
    "outputId": "34cdf6f8-7287-4c97-f875-7f4896511546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 11surQQr3jbmzHDKgEA5c46CKA23Nf7PA into data/dev.csv... Done.\n",
      "Downloading 1TokPKns1uZRAW-GDN0fUz259haU2ZZAW into data/test.csv... Done.\n",
      "Downloading 1oBiUaUHLrXehWF570xLyQNTjD9QU4fPY into data/train.csv... Done.\n",
      "Downloading 1ewgcXWBioeMVJlKsZK9gLv3hWxGDqmiz into data/trial-v3.labels... Done.\n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='11surQQr3jbmzHDKgEA5c46CKA23Nf7PA',                                 # Id of file to be downloaded\n",
    "                                    dest_path='data/dev.csv')                                                    # Destination path\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1TokPKns1uZRAW-GDN0fUz259haU2ZZAW',                                 # Id of file to be downloaded\n",
    "                                    dest_path='data/test.csv')                                                    # Destination path\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1oBiUaUHLrXehWF570xLyQNTjD9QU4fPY',                                 # Id of file to be downloaded\n",
    "                                    dest_path='data/train.csv')                                                    # Destination path\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1ewgcXWBioeMVJlKsZK9gLv3hWxGDqmiz',                                 # Id of file to be downloaded\n",
    "                                    dest_path='data/trial-v3.labels')                                                    # Destination path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbdcZz_OblvN"
   },
   "outputs": [],
   "source": [
    "# Replace the fake labels in dev file with correct labels\n",
    "\n",
    "labels = open('data/trial-v3.labels','r').readlines()\n",
    "lines = open('data/dev.csv','r').readlines()\n",
    "correct_lines=[]\n",
    "for label, line in zip(labels, lines):\n",
    "        line=line.rstrip('\\n')\n",
    "        text = line.split(',')\n",
    "        text[0]=label\n",
    "        correct_line=[''.join(text[0]),''.join(text[1:])]\n",
    "        correct_lines.append(correct_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xxNVqHJIbnZO"
   },
   "outputs": [],
   "source": [
    "# Write the clean dev data in new file\n",
    "out = open('data/clean_dev.csv',mode='w')                                      \n",
    "for line in correct_lines:\n",
    "    line[0]=line[0].rstrip('\\n')\n",
    "    out.write(line[0])\n",
    "    out.write(',')\n",
    "    out.write(line[1])\n",
    "    out.write('\\n')\n",
    "out.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Riv1NfxbnfP"
   },
   "outputs": [],
   "source": [
    "# Read train and dev data\n",
    "%%capture  # As training data has some error lines\n",
    "import pandas as pd\n",
    "X_train = pd.read_table('data/train.csv',sep=',',index_col=False, error_bad_lines=False)\n",
    "X_test = pd.read_csv('data/clean_dev.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sy7Ll0CS0Rfs"
   },
   "outputs": [],
   "source": [
    "# Pandas read skips the error lines, then we save the new dataset into separate file\n",
    "\n",
    "X_train.to_csv('data/clean_train.csv',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nOWDIvaBkqkQ"
   },
   "outputs": [],
   "source": [
    "y_train=X_train.label\n",
    "X_train = X_train.drop(['label'],axis=1)\n",
    "y_test=X_test.label1\n",
    "X_test = X_test.drop(['label1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y5sACbfI17ZT",
    "outputId": "bdfb637d-2ddf-47b3-807b-8796eab8c42e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "from skimage import transform\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter,OrderedDict\n",
    "from keras.utils.vis_utils import *\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NT9hjh34gGr9"
   },
   "outputs": [],
   "source": [
    "# Define the vector of glove embeddings\n",
    "\n",
    "%%capture  # Not display output\n",
    "from torchtext import vocab\n",
    "# specify the path to the localy saved vectors\n",
    "vec = vocab.GloVe(name='840B', dim=300)\n",
    "# vec = vocab.GloVe(name='6B', dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28EUVijcR5bD"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import spacy\n",
    "import re\n",
    "from torchtext import data\n",
    "\n",
    "# tokenizer function using spacy\n",
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s))]\n",
    "\n",
    "def tweet_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text.strip()\n",
    "\n",
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s))]\n",
    "\n",
    "# define the columns that we want to process and how to process\n",
    "TEXT = data.Field(sequential=True, \n",
    "                 tokenize=tokenizer, \n",
    "                 include_lengths=True, \n",
    "                 use_vocab=True)\n",
    "LABEL = data.Field(sequential=False, \n",
    "                   use_vocab=True,\n",
    "                   pad_token=None, \n",
    "                   unk_token=None)\n",
    "\n",
    "train_val_fields = [\n",
    "    ('label', LABEL), # process it as label\n",
    "    ('tweet', TEXT) # process it as text\n",
    "]\n",
    "\n",
    "trainds, valds = data.TabularDataset.splits(path='data', \n",
    "                                            format='csv', \n",
    "                                            train='clean_train.csv', \n",
    "                                            validation='clean_dev.csv', \n",
    "                                            fields=train_val_fields, \n",
    "                                            skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73zEBx4_lQwT"
   },
   "outputs": [],
   "source": [
    "# build the vocabulary using train and validation dataset and assign the vectors\n",
    "TEXT.build_vocab(trainds,valds, max_size=100000, vectors=vec)\n",
    "# build vocab for labels\n",
    "LABEL.build_vocab(trainds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Q05tUifp0ne0",
    "outputId": "98adfd4f-cea8-4f98-ff03-6370e2e88a7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.6339,  0.1782, -0.5805,  ...,  0.3635, -0.3022, -0.0209],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8860, -0.0697, -0.3037,  ...,  0.4366, -0.3720, -0.0778]])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PHmzIO3_85sI",
    "outputId": "c760bb16-2a70-444f-f8d4-ab739e216047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9afOJyTKk8Rv"
   },
   "outputs": [],
   "source": [
    "# Create bucketiterator to get batches\n",
    "\n",
    "traindl, valdl = data.BucketIterator.splits(datasets=(trainds, valds), # specify train and validation Tabulardataset\n",
    "                                            batch_sizes=(64,64),  # batch size of train and validation\n",
    "                                            sort_key=lambda x: len(x.tweet), # on what attribute the text should be sorted\n",
    "                                            device=None, # -1 mean cpu and 0 or None mean gpu\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N6ZGgIxc39A2",
    "outputId": "ddaff1e2-7076-4d97-cb40-932deaa727db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2365 150\n"
     ]
    }
   ],
   "source": [
    "print(len(traindl), len(valdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t9mIWH8v9Tcr",
    "outputId": "fc94152c-a6ae-459b-c157-a8837a4d2cf2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.iterator.BucketIterator at 0x7efcc9927710>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4tZudVU3_OG"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(traindl)) # BucketIterator return a batch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "PNKXAgZB4A4X",
    "outputId": "901a89a5-8ff3-435f-ae09-9e7cac5d1be8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 4, 0, 2, 1, 0, 4, 5, 2, 1, 3, 0, 0, 2, 0, 0, 3, 3, 0, 1, 1, 5, 4,\n",
      "        1, 0, 1, 5, 2, 2, 4, 1, 1, 0, 3, 4, 0, 2, 0, 4, 0, 1, 5, 5, 4, 4, 4, 0,\n",
      "        1, 0, 3, 4, 3, 1, 0, 2, 3, 3, 0, 3, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(batch.label) # labels of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1003
    },
    "colab_type": "code",
    "id": "HfWglGms4CHK",
    "outputId": "9afdd2f7-4124-4a3d-ee42-6817ed91d1af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  285,  1102,     5,     5,    68,    10,     5,     3,     5,     5,\n",
      "             5,   283,    81,     5,     5,     2,     5,    14,   326,     3,\n",
      "           106,     3,     3,     2,     5,     3,     3,    36,     5,    45,\n",
      "             3,    13,     3,   920,     7,    30,    10,     5,     3,     3,\n",
      "            14,    16,     3,     5,     3,   298,     3,     5,   104,  4089,\n",
      "             3,     3,     3,     5,    14,   205,     7,     9, 53238,     3,\n",
      "             5, 10889,     5,     5],\n",
      "        [  243,     8,     3, 12453,     2,    49,     5,    31,     3,   106,\n",
      "             3,   146,    25,    44,     2,     7,    10,     2,     3,    31,\n",
      "             2,    79,    11,    17,    26,    79,   117,     2,   176,    18,\n",
      "            11,     9,    79,     2,    26,    15,    97,     3,    11,    21,\n",
      "           145,     9,    79,     9,    21,     2,    11,    48,     2,    34,\n",
      "            31,    11,    21,    33,     2,    89,    51,    60,    40,    31,\n",
      "           324,     2,    14, 18549],\n",
      "        [    2,    23,    11,    16,     4,    14,   104,    14,    48,    32,\n",
      "            31,   389,  3504,    49,     4,  6440,   135,     4,    11,    14,\n",
      "             7,   920,    14,    19,    49,     2,     2,     4,    18,    23,\n",
      "             2,    15,   710,     4,     2,   105,    24,    11,    73,     2,\n",
      "             2,     2,    14,    15,     2,     4,     2,    14,     4,    94,\n",
      "             2,     2,    14,    16,     4,    14,    14,    24,    74,    14,\n",
      "             3,     4,     2,     2],\n",
      "        [    7,    73,     2,     2,     5,     2,     2,     2,     2,     2,\n",
      "             2,   146,  1623,     2,     6,   860,    23,     3,     2,     2,\n",
      "            12,     2,   145,     6,    39,     7,     7,  7899,     9,     2,\n",
      "             4,     2,     2,     3,     4,     2,     2,     2,     2,     4,\n",
      "             4,     4,     2,     2,     4,     5,    17,     2,  1715,     2,\n",
      "             7,     4,     2,     2,    10,     2,     2,     2,    73,     2,\n",
      "            11,     3,     4,     4],\n",
      "        [    3,     2,     7,     4,  1281,     4,     4,     7,    17,    17,\n",
      "             7,     2,     2,     4,   123,   866,     2,   541,    17,     7,\n",
      "           190,     7,     2,   186,     2,     3,    20,    43,     2,     7,\n",
      "            15,    17,    17,    11,    10,     4,     7,     4,     4,    86,\n",
      "             3,     3,    17,     4,    10,    16,     3,     4, 52683,     7,\n",
      "             3,    44,     7,     7,   282,     7,     4,     7,     2,     7,\n",
      "             2,  1917,    10,  8469],\n",
      "        [  110,     7,     3,  2038,    53,     9,  4310,    20,     3,    16,\n",
      "            10,    17,     4,    10, 67198,    29,    17, 69474,     3,     3,\n",
      "          9151,     3,     4,   958,     4,   201,   321,    32,     7,    10,\n",
      "             6,     3,    19,   143,   564,    33,  1035,    10,     3,   371,\n",
      "          1281,   318,    19,    10,    86,   771,    11,    10,    16,    92,\n",
      "            43,    49,    26,   120,    29,     3,    10,  2559,     4,     3,\n",
      "             7,    20,   102, 14567],\n",
      "        [  105,   708,    98,    56,    19,  2096,    16,  1492,    48,    69,\n",
      "           972,    81,  2929,   814,    21,    28,    92,    15,    11,   318,\n",
      "            22,    11,     9,   155,    15,    42,    16,    23,    10,   899,\n",
      "           292, 28200,    33,    12,    18,    86,    25, 62380,    48,    12,\n",
      "          1656, 12566,  1021,  1527,    32,    24,   785,   102,   143,    63,\n",
      "            18,  3251,  1014,    33,    28,  5396,   237,   249,   147,    11,\n",
      "             4,  5334,   392,   124],\n",
      "        [  577,  7449,   369,   435,   625,    24,   113,  2472,   715,   631,\n",
      "            24,    32,   247,   115,  5163,    27, 48378,   240,   715,  4520,\n",
      "           487,   393,  1114,    90,    81,    10,  2179, 25058,   117,   155,\n",
      "           387,   342,  1162,   399,   381,   319,  1716,    24,  1423,   931,\n",
      "         24737,  3433,   495,    92,  5803,   183,    10,   392, 10762,   130,\n",
      "           369,   191,  4198,   319,    27,   113,   577,  1332,    24,   715,\n",
      "           454,     1,     1,     1]]), tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7]))\n"
     ]
    }
   ],
   "source": [
    "print(batch.tweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XQkVolv04Dfs",
    "outputId": "aa3be79a-42f4-4c23-ec76-a909a4bafac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': <torchtext.data.field.Field object at 0x7efd473a5588>, 'tweet': <torchtext.data.field.Field object at 0x7efcc4df7080>}\n"
     ]
    }
   ],
   "source": [
    "print(batch.dataset.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssMm5Wp54E2D"
   },
   "outputs": [],
   "source": [
    "# Create batchgenerator class to use Bucketiterator and attributes to get the batches\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, dl, x_field, y_field):\n",
    "        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            X = getattr(batch, self.x_field)\n",
    "            y = getattr(batch, self.y_field)\n",
    "            yield (X,y)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJ1TfTlA4F7S"
   },
   "outputs": [],
   "source": [
    "train_batch_it = BatchGenerator(traindl, 'tweet', 'label')\n",
    "valid_batch_it = BatchGenerator(valdl, 'tweet', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6LKOxYP7kHQ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    " \n",
    "# Define the class for LSTM model with attention         \n",
    "    \n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_lstm_layers,emb_dim, dropout_p):\n",
    "        super(AttentionLSTM,self).__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors, requires_grad=False)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=n_lstm_layers)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1=nn.Linear(hidden_dim,128)\n",
    "        self.predictor=nn.Linear(128,6)\n",
    " \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.squeeze(0)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return new_hidden_state\n",
    "  \n",
    "    def forward(self, seq):\n",
    "        output, (final_hidden_state, final_cell_state) = self.encoder(self.embedding(seq))\n",
    "        output = output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\n",
    "        attn_output = self.attention_net(output, final_hidden_state)\n",
    "        attn_output= self.dropout(attn_output)\n",
    "        fc_output= self.fc1(attn_output)\n",
    "        fc_output= self.dropout(fc_output)\n",
    "        preds=self.predictor(fc_output)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PObJ6utqAtNF"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "nh = 256\n",
    "drop=0.5\n",
    "n_lstm_layers=1\n",
    "model = AttentionLSTM(nh,n_lstm_layers,embed_size,drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "hKn5fzsc_DQO",
    "outputId": "cc0e4025-2a62-4210-a787-836a6796fa77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.2110, Validation Loss: 0.3362, F1 Sum:93.0315,Average F1 score(Validation Set): 0.6211\n",
      "Epoch: 2, Training Loss: 0.2002, Validation Loss: 0.3428, F1 Sum:93.3294,Average F1 score(Validation Set): 0.6231\n",
      "Epoch: 3, Training Loss: 0.1953, Validation Loss: 0.3487, F1 Sum:93.0176,Average F1 score(Validation Set): 0.6210\n",
      "Epoch: 4, Training Loss: 0.1938, Validation Loss: 0.3511, F1 Sum:93.1964,Average F1 score(Validation Set): 0.6222\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(opt, step_size=1, gamma=0.5) \n",
    "epochs = 4\n",
    "batch_size = 64 \n",
    "val_list_fscores=[]\n",
    "\n",
    "state = torch.load('mytraining1.pt')\n",
    "model.load_state_dict(state)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    iterations=0\n",
    "    scheduler.step()\n",
    "    model.train() # turn on training mode\n",
    "    for x,y in tqdm(train_batch_it,disable=True): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        iterations=iterations+1\n",
    "        opt.zero_grad()\n",
    "        (x,lengths)=x\n",
    "        predicted = model(x)\n",
    "        y=y.view(-1)\n",
    "        loss = loss_func(predicted,y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    epoch_loss = running_loss / len(trainds)\n",
    " \n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    list_scores=[]\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x,y in tqdm(valid_batch_it,disable=True):\n",
    "        (x,lengths)=x\n",
    "        preds = model(x)\n",
    "        y=y.view(-1)\n",
    "        loss = loss_func(preds,y)\n",
    "        val_loss += loss.item() * x.size(0)\n",
    "        _,y_pred=torch.max(preds,1)\n",
    "        temp=f1_score(y_pred,y, average='macro', labels=np.unique(y_pred))\n",
    "        list_scores.append(temp)\n",
    "    \n",
    "    val_loss /= len(valds)\n",
    "    fscore = sum(list_scores)/(len(valds)/batch_size)\n",
    "    val_list_fscores.append(fscore)\n",
    "    torch.save(model.state_dict(), 'mytraining'+str(epoch)+'.pt')\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, F1 Sum:{:.4f},Average F1 score(Validation Set): {:.4f}'.format(epoch, epoch_loss, val_loss,sum(list_scores),fscore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RPzYXTZHFFhw"
   },
   "outputs": [],
   "source": [
    "# Save the model with highest F score\n",
    "\n",
    "index = np.argmax(val_list_fscores)\n",
    "from google.colab import files\n",
    "files.download('mytraining'+str(index+1)+'.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "NLTK: Stemming and Lemmatization<br\\>\n",
    "https://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    "\n",
    "Apply Stemmer to a column<br\\>\n",
    "https://stackoverflow.com/questions/43795310/apply-porters-stemmer-to-a-pandas-column-for-each-word\n",
    "\n",
    "Ensemble: Voting Classifier<br\\>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "\n",
    "Sample Pipeline<br\\>\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "\n",
    "TorchText and LSTM<br\\>\n",
    "http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "\n",
    "TorchText and CNN<br\\>\n",
    "https://github.com/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/08.CNN-for-Text-Classification.ipynb\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "\n",
    "Saving and loading Pytorch Model<br\\>\n",
    "https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
    "\n",
    "Self Attention Explained<br\\>\n",
    "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#self-attention\n",
    "\n",
    "Self Attention Code for LSTM<br\\>\n",
    "https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/models/LSTM_Attn.py\n",
    "\n",
    "LSTM + CNN model \n",
    "https://discuss.pytorch.org/t/cnn-layer-in-the-top-of-lstm/7941/5\n",
    "https://discuss.pytorch.org/t/solved-concatenate-time-distributed-cnn-with-lstm/15435/4"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_attention2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
